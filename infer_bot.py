import os
import shutil
import time
import cv2
import numpy as np
from PIL import Image
import torch
import easyocr
from telegram import Update
from telegram.ext import ApplicationBuilder, MessageHandler, ContextTypes, filters
import nest_asyncio
import asyncio

# –ü—Ä–∏–º–µ–Ω—è–µ–º nest_asyncio, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ "Cannot close a running event loop"
nest_asyncio.apply()

# === CONFIG ===
BOT_TOKEN = (
    "7287622548:AAGBEwjd5nhQS-XhGv4sa6Ihc06LOfZlHM4"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π —Ä–µ–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
)


# –ú–æ–¥–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ (–∏–∑ –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã)
class Opt:
    pass


opt = Opt()
opt.Transformation = "TPS"
opt.FeatureExtraction = "ResNet"
opt.SequenceModeling = "BiLSTM"
opt.Prediction = "Attn"
opt.character = " !%'()*+,-./0123456789:;<=>[]^_v{|}~¬ß¬´¬ª–ê–ë–í–ì–î–ï–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è—ë—ñ—£—≥—µ‚Ññ"
opt.batch_max_length = 25
opt.imgH = 128
opt.imgW = 512
opt.input_channel = 1  # –ï—Å–ª–∏ rgb=False, —Ç–æ 1; –∏–Ω–∞—á–µ 3
opt.output_channel = 512
opt.hidden_size = 256
opt.rgb = False
opt.PAD = True
opt.num_fiducial = 20
opt.num_class = len(opt.character)
opt.sensitive = False
opt.data_filtering_off = True
opt.saved_model = r"C:\best_accuracy.pth"  # –ü—É—Ç—å –∫ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–µ–∑–∫–æ–≤ (crops) ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ—á–∏—â–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
CROPS_DIR = os.path.join(os.getcwd(), "crops")
if os.path.exists(CROPS_DIR):
    shutil.rmtree(CROPS_DIR)
os.makedirs(CROPS_DIR, exist_ok=True)


# ==== –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ====
def load_model(opt):
    from model import Model
    from utils import CTCLabelConverter, AttnLabelConverter

    print("[DEBUG] –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    # –î–ª—è –º–æ–¥–µ–ª–∏ Attn –∏—Å–ø–æ–ª—å–∑—É–µ–º AttnLabelConverter
    converter = AttnLabelConverter(opt.character)
    opt.num_class = len(converter.character)

    model = Model(opt)
    model = torch.nn.DataParallel(model).to(device)
    state_dict = torch.load(opt.saved_model, map_location=device)
    print("[DEBUG] –ó–∞–≥—Ä—É–∑–∫–∞ state_dict –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    if "module" in list(state_dict.keys())[0]:
        model.load_state_dict(state_dict)
    else:
        model.load_state_dict({"module." + k: v for k, v in state_dict.items()})
    model = model.module
    model.eval()
    print("[DEBUG] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ eval mode")
    return model, converter


# ==== –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ AlignCollate (–∏–∑ PIL Image) ====
def preprocess_image_from_pil(pil_img, opt):
    from dataset import AlignCollate

    print("[DEBUG] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ AlignCollate...")
    align = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)
    image_tensor, _ = align([(pil_img.convert("RGB" if opt.rgb else "L"), "")])
    print(f"[DEBUG] –¢–µ–Ω–∑–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_tensor.shape}")
    return image_tensor.to(device)


# ==== –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –±–ª–æ–∫–µ ====
def predict(model, converter, image_tensor, opt):
    print("[DEBUG] –ó–∞–ø—É—Å–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è...")
    batch_size = image_tensor.size(0)
    length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)
    text_for_pred = (
        torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)
    )
    with torch.no_grad():
        preds = model(image_tensor, text_for_pred, is_train=False)
        _, preds_index = preds.max(2)
        preds_str = converter.decode(preds_index, length_for_pred)
        preds_str = [s.split("[s]")[0] for s in preds_str]
    recognized = preds_str[0]
    print("[DEBUG] –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return recognized


# ==== –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏) ====
def process_full_image(image_path, opt, crops_dir):
    model, converter = load_model(opt)
    reader = easyocr.Reader(["ru", "en"], gpu=torch.cuda.is_available())
    print("[DEBUG] –°—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenCV...")
    image = cv2.imread(image_path)

    # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ (EasyOCR)
    start_detection = time.time()
    detections = reader.readtext(image)
    detection_time = time.time() - start_detection
    print(
        f"[DEBUG] –ù–∞–π–¥–µ–Ω–æ {len(detections)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤. –í—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏: {detection_time:.2f} —Å–µ–∫."
    )

    predictions = []
    total_recognition_time = 0
    recognized_count = 0

    for i, (bbox, _, confidence) in enumerate(detections):
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ –±–ª–æ–∫–∏, confidence >= 0.0)
        if confidence < 0.0:
            print(f"[DEBUG] –ë–ª–æ–∫ {i:03} –ø—Ä–æ–ø—É—â–µ–Ω (confidence {confidence:.2f})")
            continue

        pts = np.array(bbox).astype(int)
        x, y, w, h = cv2.boundingRect(pts)
        cropped_img = image[y : y + h, x : x + w]

        crop_path = os.path.join(crops_dir, f"word_{i:03}.png")
        cv2.imwrite(crop_path, cropped_img)
        print(f"[DEBUG] –°–æ—Ö—Ä–∞–Ω—ë–Ω crop {crop_path}")

        pil_crop = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
        image_tensor_crop = preprocess_image_from_pil(pil_crop, opt)

        start_recognition = time.time()
        prediction = predict(model, converter, image_tensor_crop, opt)
        recognition_time = time.time() - start_recognition
        total_recognition_time += recognition_time
        recognized_count += 1

        predictions.append(prediction)
        print(f"[{i:03}] ‚Üí {prediction} ({recognition_time*1000:.2f} –º—Å)")

    final_text = " ".join(predictions)
    print("\nüìú –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
    print(final_text)
    print(f"\n‚è± –í—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏: {detection_time:.2f} —Å–µ–∫")
    print(
        f"‚è± –°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –±–ª–æ–∫–æ–≤: {total_recognition_time*1000:.2f} –º—Å"
    )
    return final_text, predictions, detection_time, total_recognition_time


# ==== –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Ñ–æ—Ç–æ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ñ–∞–π–ª–æ–≤) –≤ Telegram –±–æ—Ç–µ ====
async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    print("[DEBUG] –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ñ–æ—Ç–æ")
    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    local_path = os.path.join(os.getcwd(), "received.jpg")
    print(f"[DEBUG] –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ –≤ {local_path}...")
    await file.download_to_drive(local_path)
    print("[DEBUG] –§–æ—Ç–æ —Å–∫–∞—á–∞–Ω–æ")

    start = time.time()
    final_text, _, detection_time, total_recognition_time = process_full_image(
        local_path, opt, CROPS_DIR
    )
    total_duration = time.time() - start

    reply = (
        f"üìú –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{final_text}\n\n"
        f"‚è± –í—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏: {detection_time:.2f} —Å–µ–∫\n"
        f"‚è± –°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {total_recognition_time*1000:.2f} –º—Å\n"
        f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_duration:.2f} —Å–µ–∫"
    )
    print(f"[DEBUG] –û—Ç–≤–µ—Ç: {reply}")
    await update.message.reply_text(reply)


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    document = update.message.document
    if not document.mime_type.startswith("image/"):
        await update.message.reply_text(
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º."
        )
        return

    print("[DEBUG] –ü–æ–ª—É—á–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
    file = await context.bot.get_file(document.file_id)
    local_path = os.path.join(os.getcwd(), "received.jpg")
    print(f"[DEBUG] –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ {local_path}...")
    await file.download_to_drive(local_path)
    print("[DEBUG] –î–æ–∫—É–º–µ–Ω—Ç —Å–∫–∞—á–∞–Ω")

    start = time.time()
    final_text, _, detection_time, total_recognition_time = process_full_image(
        local_path, opt, CROPS_DIR
    )
    total_duration = time.time() - start

    reply = (
        f"üìú –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{final_text}\n\n"
        f"‚è± –í—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏: {detection_time:.2f} —Å–µ–∫\n"
        f"‚è± –°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {total_recognition_time*1000:.2f} –º—Å\n"
        f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_duration:.2f} —Å–µ–∫"
    )
    print(f"[DEBUG] –û—Ç–≤–µ—Ç: {reply}")
    await update.message.reply_text(reply)


# ==== –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ====
async def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    app.add_handler(MessageHandler(filters.Document.IMAGE, handle_document))
    print("[DEBUG] –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–π...")
    await app.run_polling(close_loop=False)


if __name__ == "__main__":
    asyncio.run(main())
