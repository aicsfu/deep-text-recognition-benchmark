import os
import shutil
import cv2
import numpy as np
from PIL import Image
import torch
import easyocr
from model import Model
from utils import CTCLabelConverter, AttnLabelConverter
from dataset import AlignCollate

# ==== –ù–ê–°–¢–†–û–ô–ö–ò ====
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

image_path = r"C:\A\a2\combined_images\1726.jpg"  # –ü—É—Ç—å –∫ –ø–æ–ª–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
saved_model = r"C:\best_accuracy.pth"

# –ü–∞–ø–∫–∞ –¥–ª—è –æ–±—Ä–µ–∑–∫–æ–≤
output_crop_dir = os.path.join(os.getcwd(), "crops")
if os.path.exists(output_crop_dir):
    shutil.rmtree(output_crop_dir)
os.makedirs(output_crop_dir, exist_ok=True)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
Transformation = "TPS"
FeatureExtraction = "ResNet"
SequenceModeling = "BiLSTM"
Prediction = "Attn"

character = " !%'()*+,-./0123456789:;<=>[]^_v{|}~¬ß¬´¬ª–ê–ë–í–ì–î–ï–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è—ë—ñ—£—≥—µ‚Ññ"
batch_max_length = 25
imgH = 128
imgW = 512
input_channel = 1
output_channel = 512
hidden_size = 256
rgb = False
PAD = True


# ==== –ö–û–ù–§–ò–ì ====
class Opt:
    pass


opt = Opt()
opt.Transformation = Transformation
opt.FeatureExtraction = FeatureExtraction
opt.SequenceModeling = SequenceModeling
opt.Prediction = Prediction
opt.character = character
opt.batch_max_length = batch_max_length
opt.imgH = imgH
opt.imgW = imgW
opt.input_channel = 3 if rgb else 1
opt.output_channel = output_channel
opt.hidden_size = hidden_size
opt.saved_model = saved_model
opt.rgb = rgb
opt.PAD = PAD
opt.num_fiducial = 20
opt.num_class = len(character)
opt.sensitive = False
opt.data_filtering_off = True


# ==== –ú–æ–¥–µ–ª—å ====
def load_model(opt):
    if "CTC" in opt.Prediction:
        converter = CTCLabelConverter(opt.character)
    else:
        converter = AttnLabelConverter(opt.character)
    opt.num_class = len(converter.character)

    model = Model(opt)
    model = torch.nn.DataParallel(model).to(device)
    state_dict = torch.load(opt.saved_model, map_location=device)
    if "module" in list(state_dict.keys())[0]:
        model.load_state_dict(state_dict)
    else:
        model.load_state_dict({"module." + k: v for k, v in state_dict.items()})
    model = model.module
    model.eval()
    return model, converter


# ==== –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ ====
def preprocess_image_from_pil(pil_img, opt):
    align = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)
    image_tensor, _ = align([(pil_img.convert("RGB" if opt.rgb else "L"), "")])
    return image_tensor.to(device)


# ==== –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ====
def predict(model, converter, image_tensor, opt):
    batch_size = image_tensor.size(0)
    length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)
    text_for_pred = (
        torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)
    )

    with torch.no_grad():
        if "CTC" in opt.Prediction:
            preds = model(image_tensor, text_for_pred)
            preds_size = torch.IntTensor([preds.size(1)] * batch_size)
            _, preds_index = preds.max(2)
            preds_str = converter.decode(preds_index, preds_size)
        else:
            preds = model(image_tensor, text_for_pred, is_train=False)
            _, preds_index = preds.max(2)
            preds_str = converter.decode(preds_index, length_for_pred)
            preds_str = [s.split("[s]")[0] for s in preds_str]

    return preds_str[0]


# ==== –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ====
def process_full_image(image_path, opt, output_crop_dir):
    model, converter = load_model(opt)
    reader = easyocr.Reader(["ru", "en"], gpu=torch.cuda.is_available())
    image = cv2.imread(image_path)
    detections = reader.readtext(image)

    print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ {len(detections)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤...\n")

    predictions = []

    for i, (bbox, _, confidence) in enumerate(detections):
        if confidence < 0.0:
            continue

        pts = np.array(bbox).astype(int)
        x, y, w, h = cv2.boundingRect(pts)
        cropped_img = image[y : y + h, x : x + w]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ –æ–±—Ä–µ–∑–æ–∫
        crop_path = os.path.join(output_crop_dir, f"word_{i:03}.png")
        cv2.imwrite(crop_path, cropped_img)

        pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
        image_tensor = preprocess_image_from_pil(pil_img, opt)
        prediction = predict(model, converter, image_tensor, opt)

        predictions.append(prediction)

    # –°–∫–ª–µ–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥
    final_text = " ".join(predictions)
    print("\nüìú –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
    print(final_text)


# ==== –°–¢–ê–†–¢ ====
if __name__ == "__main__":
    process_full_image(image_path, opt, output_crop_dir)
